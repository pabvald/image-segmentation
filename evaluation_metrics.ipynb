{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Evaluation metrics for the Cityscapes dataset\n",
    "In this Jupyter Notebook we show how to compute and visualize different evaluation metrics for the Cityscapes Pixel-Segmentation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cityscapesscripts\n",
      "  Downloading cityscapesScripts-2.2.0-py3-none-any.whl (472 kB)\n",
      "\u001b[K     |████████████████████████████████| 472 kB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: appdirs in /opt/conda/lib/python3.6/site-packages (from cityscapesscripts) (1.4.4)\n",
      "Collecting pyquaternion\n",
      "  Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from cityscapesscripts) (1.18.5)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 6.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from cityscapesscripts) (3.2.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from cityscapesscripts) (4.31.1)\n",
      "Requirement already satisfied: typing in /opt/conda/lib/python3.6/site-packages (from cityscapesscripts) (3.7.4.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.6/site-packages (from cityscapesscripts) (8.1.2)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-9.1-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->cityscapesscripts) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->cityscapesscripts) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->cityscapesscripts) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->cityscapesscripts) (2.8.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from cycler>=0.10->matplotlib->cityscapesscripts) (1.15.0)\n",
      "Installing collected packages: pyquaternion, humanfriendly, coloredlogs, cityscapesscripts\n",
      "Successfully installed cityscapesscripts-2.2.0 coloredlogs-15.0 humanfriendly-9.1 pyquaternion-0.9.9\n"
     ]
    }
   ],
   "source": [
    "!pip install cityscapesscripts;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.metrics\n",
    "from importlib import reload\n",
    "from os.path import join as pjoin\n",
    "from loaders.cityscapes import cityscapesDataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"./Cityscapes\"\n",
    "pckgs_names = [\"gtFine_trainvaltest.zip\",\"leftImg8bit_trainvaltest.zip\"]\n",
    "dir_names = [\"gtFine\", \"leftImg8bit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "We assume that the datset *Cityscapes* has already been downloaded in the folder *local_path*. If this is not the case, check first the notebook [Exploring the Cityscapes dataset](./cityscapes_dataset.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations files processed\n"
     ]
    }
   ],
   "source": [
    "training_data = cityscapesDataset(local_path, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 255]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = training_data.label_ids()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1310720])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [training_data[i][1] for i in range(0,10)]\n",
    "y_true = torch.flatten(torch.cat(y_true))\n",
    "y_true.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1310720])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [training_data[i][1] for i in range(15,25)]\n",
    "y_pred = torch.flatten(torch.cat(y_pred))\n",
    "y_pred.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'evaluation' from '/project/simple-imageseg/evaluation.py'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import evaluation\n",
    "importlib.reload(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an evaluation report providing a dataloader and a model to make the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an evaluation report providing the ground-truth values and the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_report = evaluation.EvaluationReport.from_predictions(y_true.numpy(), y_pred.numpy(), labels=list(range(19)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the metrics for a single targe class, in a 'one vs. rest' way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7463,\n",
       " 'sensitivity': 0.6746,\n",
       " 'specificity': 0.7672,\n",
       " 'dice_coeff': 0.5455,\n",
       " 'jaccard_sim': 0.375,\n",
       " 'f1_score': 0.5455}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_class = 2\n",
    "metrics2 = eval_report.get_metrics(pos_label=target_class)\n",
    "metrics2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of decimal places can be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.75,\n",
       " 'sensitivity': 0.67,\n",
       " 'specificity': 0.77,\n",
       " 'dice_coeff': 0.55,\n",
       " 'jaccard_sim': 0.38,\n",
       " 'f1_score': 0.55}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_report.decimal_places = 2\n",
    "eval_report.get_metrics(pos_label=target_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also obtain the average metrics for all classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.89,\n",
       " 'sensitivity': 0.42,\n",
       " 'specificity': 0.91,\n",
       " 'dice_coeff': 0.25,\n",
       " 'jaccard_sim': 0.14,\n",
       " 'f1_score': 0.25}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_report.get_metrics(pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.95,\n",
       " 'sensitivity': 0.11,\n",
       " 'specificity': 0.97,\n",
       " 'dice_coeff': 0.1,\n",
       " 'jaccard_sim': 0.06,\n",
       " 'f1_score': 0.1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_report.get_metrics(average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can include the weight of each class to compute a weighted average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.94654,\n",
       " 'sensitivity': 0.13149,\n",
       " 'specificity': 0.96806,\n",
       " 'dice_coeff': 0.10474,\n",
       " 'jaccard_sim': 0.06738,\n",
       " 'f1_score': 0.10474}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_report.weights = np.random.rand(19)\n",
    "eval_report.decimal_places = 5\n",
    "eval_report.get_metrics(average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to compute all the metrics. We can select which metrics we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.94756, 'dice_coeff': 0.099}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_report.get_metrics(metrics=['accuracy', 'dice_coeff'], average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or compute a single metric directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8929"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_report.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25125"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_report.f1_score()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
